{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Temporal Difference learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab is divided into two sections, in the first part (based on the [reinforcement learning](https://www.neuromatchacademy.org/syllabus) neuromatch exercise) you will learn how to estimate state-value functions in a classical conditioning paradigm using Temporal Difference (TD) learning and examine TD-errors at the presentation of the conditioned and unconditioned stimulus (CS and US) under different CS-US contingencies. \n",
    "In the second part you will implement Q-learning algorithm in a grid-based map where the agent has to reach a target location.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) TD error and learning\n",
    "\n",
    "You will work with the classical conditioning environment which is composed of a sequence of states that the agent deterministically transitions through. Starting at State 0, the agent moves to State 1 in the first step, from State 1 to State 2 in the second, and so on. These states represent time in the tapped delay line representation.\n",
    "\n",
    "Within each episode, the agent is presented with a CS and an US (reward).\n",
    "\n",
    "The CS is always presented at 1/4 of the total duration of the trial. The US (reward) is then delivered after the CS. The interval between the CS and the US is specified by reward_time.\n",
    "\n",
    "The agent's goal is to learn to predict expected rewards from each state in the trial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "\n",
    "Let's run the following cells to import the libraries and the helper functions for the classical conditioning environment. As the first part of the lab contains interactive exercises, you need `ipywidgets` which you can install with conda:\n",
    "\n",
    "```python\n",
    "conda install -c conda-forge ipywidgets\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets  # interactive display\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Helper functions\n",
    "from matplotlib import ticker\n",
    "\n",
    "def plot_value_function(V, ax=None, show=True):\n",
    "  \"\"\"Plot V(s), the value function\"\"\"\n",
    "  if not ax:\n",
    "    fig, ax = plt.subplots(figsize=(10,8))\n",
    "\n",
    "  ax.stem(V, use_line_collection=True)\n",
    "  ax.set_ylabel('Value')\n",
    "  ax.set_xlabel('State')\n",
    "  ax.set_title(\"Value function: $V(s)$\")\n",
    "\n",
    "  if show:\n",
    "    plt.show()\n",
    "\n",
    "def plot_tde_trace(TDE, ax=None, show=True, skip=400):\n",
    "  \"\"\"Plot the TD Error across trials\"\"\"\n",
    "  if not ax:\n",
    "    fig, ax = plt.subplots(figsize=(10,8))\n",
    "\n",
    "  indx = np.arange(0, TDE.shape[1], skip)\n",
    "  im = ax.imshow(TDE[:,indx])\n",
    "  positions = ax.get_xticks()\n",
    "  # Avoid warning when setting string tick labels\n",
    "  ax.xaxis.set_major_locator(ticker.FixedLocator(positions))\n",
    "  ax.set_xticklabels([f\"{int(skip * x)}\" for x in positions])\n",
    "  ax.set_title('TD-error over learning')\n",
    "  ax.set_ylabel('State')\n",
    "  ax.set_xlabel('Iterations')\n",
    "  ax.figure.colorbar(im).set_label('TD-error')\n",
    "  if show:\n",
    "    plt.show()\n",
    "\n",
    "def learning_summary_plot(V, TDE):\n",
    "  \"\"\"Summary plot for Ex1\"\"\"\n",
    "  fig, (ax1, ax2) = plt.subplots(figsize=(10,8), nrows = 2, gridspec_kw={'height_ratios': [1, 2]})\n",
    "\n",
    "  plot_value_function(V, ax=ax1, show=False)\n",
    "  plot_tde_trace(TDE, ax=ax2, show=False)\n",
    "  plt.tight_layout()\n",
    "\n",
    "#@title Default title text\n",
    "class ClassicalConditioning:\n",
    "\n",
    "    def __init__(self, n_steps, reward_magnitude, reward_time):\n",
    "\n",
    "        # Task variables\n",
    "        self.n_steps = n_steps\n",
    "        self.n_actions = 0\n",
    "        self.cs_time = int(n_steps/4) - 1\n",
    "\n",
    "        # Reward variables\n",
    "        self.reward_state = [0,0]\n",
    "        self.reward_magnitude = None\n",
    "        self.reward_probability = None\n",
    "        self.reward_time = None\n",
    "\n",
    "        self.set_reward(reward_magnitude, reward_time)\n",
    "\n",
    "        # Time step at which the conditioned stimulus is presented\n",
    "\n",
    "        # Create a state dictionary\n",
    "        self._create_state_dictionary()\n",
    "\n",
    "    def set_reward(self, reward_magnitude, reward_time):\n",
    "\n",
    "        \"\"\"\n",
    "        Determine reward state and magnitude of reward\n",
    "        \"\"\"\n",
    "        if reward_time >= self.n_steps - self.cs_time:\n",
    "            self.reward_magnitude = 0\n",
    "\n",
    "        else:\n",
    "            self.reward_magnitude = reward_magnitude\n",
    "            self.reward_state = [1, reward_time]\n",
    "\n",
    "    def get_outcome(self, current_state):\n",
    "\n",
    "        \"\"\"\n",
    "        Determine next state and reward\n",
    "        \"\"\"\n",
    "        # Update state\n",
    "        if current_state < self.n_steps - 1:\n",
    "            next_state = current_state + 1\n",
    "        else:\n",
    "            next_state = 0\n",
    "\n",
    "        # Check for reward\n",
    "        if self.reward_state == self.state_dict[current_state]:\n",
    "            reward = self.reward_magnitude\n",
    "        else:\n",
    "            reward = 0\n",
    "\n",
    "        return next_state, reward\n",
    "\n",
    "    def _create_state_dictionary(self):\n",
    "\n",
    "        \"\"\"\n",
    "        This dictionary maps number of time steps/ state identities\n",
    "        in each episode to some useful state attributes:\n",
    "\n",
    "        state      - 0 1 2 3 4 5 (cs) 6 7 8 9 10 11 12 ...\n",
    "        is_delay   - 0 0 0 0 0 0 (cs) 1 1 1 1  1  1  1 ...\n",
    "        t_in_delay - 0 0 0 0 0 0 (cs) 1 2 3 4  5  6  7 ...\n",
    "        \"\"\"\n",
    "        d = 0\n",
    "\n",
    "        self.state_dict = {}\n",
    "        for s in range(self.n_steps):\n",
    "            if s <= self.cs_time:\n",
    "                self.state_dict[s] = [0,0]\n",
    "            else:\n",
    "                d += 1 # Time in delay\n",
    "                self.state_dict[s] = [1,d]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main concepts\n",
    "\n",
    "We start by defining the Return $G_t$ or the sum of all the rewards starting from time $t$:\n",
    "$$ G_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1} = r_{t+1} + \\gamma G_{t+1}$$\n",
    "where $\\gamma [0,1]$  is the discount factor and controls the importance of the future rewards.\n",
    "Although we do not know the value of the Return at time $t$, the agent's goal is to make a learned estimation of it starting from any given state. This is also known as the Value function $V_{\\pi}(s_t=s)$ or the expectation of the return starting from state $s$ and following a policy $\\pi$ (the policy tells which action to take at each state $s$). More formally:\n",
    "$$ V_{\\pi}(s_t=s) = E[G_t | s_t = s, a_t \\sim \\pi] $$\n",
    "$$ = E[r_{t+1} + \\gamma G_{t+1}| s_t = s, a_t \\sim \\pi]$$\n",
    "$$ = E[r_{t+1} + \\gamma V_{\\pi}(s_{t+1})| s_t = s, a_t \\sim \\pi]$$\n",
    "\n",
    "As you can see, the Value depends on the Value at the next time step in a recursive manner. Since we don't know the true value $G_{t+1}$, thanks to the Markov assumption, we can bootstrap $V(s_{t+1})$ and use it to replace $G_{t+1}$.\n",
    "This difference between the value at the next time step and the current time step is called the Temporal Difference (TD)-error:\n",
    "$$\\delta_t = r_{t+1} + \\gamma V_{\\pi}(s_{t+1}) - V(s_t)$$\n",
    "\n",
    "Hence, the update rule for the value can be written as:\n",
    "$$ V(s_t) = V(s_t) + \\alpha \\delta_t$$\n",
    "where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1) TD-learning with guaranteed rewards\n",
    "\n",
    "Implement TD-learning to estimate the state-value function in the classical-conditioning world with guaranteed rewards, with a fixed magnitude, at a fixed delay after the conditioned stimulus, CS. Save TD-errors over learning (i.e., over trials) so we can visualise them afterwards.\n",
    "\n",
    "In order to simulate the effect of the CS, you should only update $V(s_t)$\n",
    " during the delay period after CS. This period is indicated by the boolean variable ```is_delay```. This can be implemented by multiplying the expression for updating the value function by ```is_delay```.\n",
    "\n",
    "Use the provided code to estimate the value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the lines to calculate TD-error and to update the value V\n",
    "def td_learner(env, n_trials, gamma=0.98, alpha=0.001):\n",
    "  \"\"\" Temporal Difference learning\n",
    "\n",
    "  Args:\n",
    "    env (object): the environment to be learned\n",
    "    n_trials (int): the number of trials to run\n",
    "    gamma (float): temporal discount factor\n",
    "    alpha (float): learning rate\n",
    "\n",
    "  Returns:\n",
    "    ndarray, ndarray: the value function and temporal difference error arrays\n",
    "  \"\"\"\n",
    "  V = np.zeros(env.n_steps) # Array to store values over states (time)\n",
    "  TDE = np.zeros((env.n_steps, n_trials)) # Array to store TD errors\n",
    "\n",
    "  for n in range(n_trials):\n",
    "    state = 0 # Initial state\n",
    "    for t in range(env.n_steps):\n",
    "      # Get next state and next reward\n",
    "      next_state, reward = env.get_outcome(state)\n",
    "      # Is the current state in the delay period (after CS)?\n",
    "      is_delay = env.state_dict[state][0]\n",
    "        \n",
    "      #######################################################\n",
    "      ##\n",
    "      ## Complete the following lines \n",
    "      ##\n",
    "      #######################################################\n",
    "      # Write an expression to compute the TD-error\n",
    "      TDE[state, n] = 'TODO'\n",
    "\n",
    "      # Write an expression to update the value function\n",
    "      V[state] += 'TODO'\n",
    "\n",
    "      # Update state\n",
    "      state = 'TODO'\n",
    "\n",
    "  return V, TDE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to execute the TD-learner and to visualise the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ClassicalConditioning(n_steps=40, reward_magnitude=10, reward_time=10)\n",
    "V, TDE = td_learner(env, n_trials=20000)\n",
    "learning_summary_plot(V, TDE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2) US to CS Transfer\n",
    "\n",
    "During classical conditioning, the subject's behavioral response (e.g., salivating) transfers from the unconditioned stimulus (US; like the smell of tasty food) to the conditioned stimulus (CS) that predicts it. Reward prediction errors play an important role in this process by adjusting the value of states according to their expected, discounted return.\n",
    "\n",
    "Use the widget below to examine how reward prediction errors change over time. Before training (orange line), only the reward state has high reward prediction error. As training progresses (blue line, slider), the reward prediction errors shift to the conditioned stimulus, where they end up when the trial is complete (green line).\n",
    "\n",
    "Dopamine neurons, which are thought to carry reward prediction errors in vivo, show [notably similar behavior](https://www.nature.com/articles/nn0898_304)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "\n",
    "#@markdown Make sure you execute this cell to enable the widget!\n",
    "\n",
    "n_trials = 20000\n",
    "\n",
    "@widgets.interact\n",
    "def plot_tde_by_trial(trial = widgets.IntSlider(value=5000, min=0, max=n_trials-1 , step=1, description=\"Trial #\")):\n",
    "  if 'TDE' not in globals():\n",
    "    print(\"Complete Exercise 1 to enable this interactive demo!\")\n",
    "  else:\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    ax.axhline(0, color='k') # Use this + basefmt=' ' to keep the legend clean.\n",
    "    ax.stem(TDE[:, 0], linefmt='C1-', markerfmt='C1d', basefmt=' ',\n",
    "            label=\"Before Learning (Trial 0)\",\n",
    "            use_line_collection=True)\n",
    "    ax.stem(TDE[:, -1], linefmt='C2-', markerfmt='C2s', basefmt=' ',\n",
    "            label=\"After Learning (Trial $\\infty$)\",\n",
    "            use_line_collection=True)\n",
    "    ax.stem(TDE[:, trial], linefmt='C0-', markerfmt='C0o', basefmt=' ',\n",
    "            label=f\"Trial {trial}\",\n",
    "            use_line_collection=True)\n",
    "\n",
    "    ax.set_xlabel(\"State in trial\")\n",
    "    ax.set_ylabel(\"TD Error\")\n",
    "    ax.set_title(\"Temporal Difference Error by Trial\")\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3) Learning Rates and Discount Factors\n",
    "\n",
    "Our TD-learning agent has two parameters that control how it learns: $\\alpha$, the learning rate, and $\\gamma$\n",
    ", the discount factor. In Exercise 1, we set these parameters to $\\alpha=0.001$\n",
    "and $\\gamma=0.98$ for you. Here, you'll investigate how changing these parameters alters the model that TD-learning learns.\n",
    "\n",
    "Before enabling the interactive demo below, take a moment to think about the functions of these two parameters. $\\alpha$ controls the size of the Value function updates produced by each TD-error. In our simple, deterministic world, will this affect the final model we learn? Is a larger $\\alpha$ necessarily better in more complex, realistic environments?\n",
    "\n",
    "The discount rate $\\gamma$ applies an exponentially-decaying weight to returns occuring in the future, rather than the present timestep. How does this affect the model we learn? What happens when $\\gamma=0$\n",
    " or $\\gamma≥1$ ?\n",
    "\n",
    "Use the widget to test your hypotheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "\n",
    "#@markdown Make sure you execute this cell to enable the widget!\n",
    "\n",
    "@widgets.interact\n",
    "def plot_summary_alpha_gamma(alpha = widgets.FloatSlider(value=0.0001, min=0.0001, max=0.1, step=0.0001, readout_format='.4f', description=\"alpha\"),\n",
    "                             gamma = widgets.FloatSlider(value=0.980, min=0, max=1.1, step=0.010, description=\"gamma\")):\n",
    "  env = ClassicalConditioning(n_steps=40, reward_magnitude=10, reward_time=10)\n",
    "  try:\n",
    "    V_params, TDE_params = td_learner(env, n_trials=20000, gamma=gamma, alpha=alpha)\n",
    "  except NotImplementedError:\n",
    "    print(\"Finish Exercise 1 to enable this interactive demo\")\n",
    "\n",
    "  learning_summary_plot(V_params,TDE_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap\n",
    "\n",
    "In this notebook, we have developed a simple TD Learner and examined how its state representations and reward prediction errors evolve during training. By manipualting its environment and parameters (α\n",
    ", γ\n",
    "), you developed an intuition for how it behaves.\n",
    "\n",
    "This simple model closely resembles the behavior of subjects undergoing classical conditioning tasks and the dopamine neurons that may underlie that behavior. You may have implemented TD-reset or used the model to recreate a common experimental error. The update rule used here has been extensively studied for more than 70 years as a possible explanation for artificial and biological learning.\n",
    "\n",
    "However, you may have noticed that something is missing from this notebook. We carefully calculated the Value of each state, but did not use it to actually do anything. Using Values to plan Actions is coming up next!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Q-learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Values are related to each state as a whole, we now introduce Q-Value which is the value of each state-action pair. Q-value allows to measure the quality of each action from a given state. Hence, choosing an action corresponding to higher Q-value can improve the agent's planning.\n",
    "Among the various algorithms, Q-learning utilises the temporal-difference learning to directly approximate the optimal action-value function (that is, the action-value function of the optimal policy), no matter what policy is followed. The pseudocode taken from the book by Sutton and Barto is presented below:\n",
    "\n",
    "<img src=\"q-learning.png\" width=\"600\">\n",
    "\n",
    "Q-learning is an off-policy algorithm as two different policy are utilised to behave and to update the Q-value.\n",
    "In particular, the behavioural policy (the actions chosen by the agent while moving in the environment) is $\\epsilon - greedy$ which is different from the policy utilised for the bootstrapped Q-value $Q(S',a)$. The policy of $Q(S',a)$ is $greedy$ as it choses the max action.\n",
    "This off-policy nature can help to explore better as the agent is not constrained to the same policy while navigating the environment and while updating the Q-value $Q(S,A)$.\n",
    "\n",
    "The next task consists of implementing the Q-learning algorithm for an agent navigating a grid-world environment. The agent's aim is to reach the target grid-location. Moreover the environment presents walls where the agent cannot stay. The figure below shows the environment as well as the agent and the target. The starting point is always top-left corner.\n",
    "\n",
    "<img src=\"env.png\" width=\"400\">\n",
    "\n",
    "The walls are represented by the purple color, the agent's starting position is the top-left corner and the target is the green grid. The agent's position is the dark blue cell. The yellow grids are the valid parts where the agent can navigate.\n",
    "\n",
    "The following cell contains the class `MDP` which implements the environment class for the grid-world. The cell has been already implemented, just make sure to run before proceeding to the next where you will implement parts of the Q-learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy \n",
    "\n",
    "class MDP:\n",
    "    def __init__(self):\n",
    "        \n",
    "        #possible states\n",
    "        self.S = [(x,y) for x in range(6) for y in range(6)]\n",
    "        \n",
    "        # possible actions \n",
    "        self.A = [\"UP\",\"DOWN\",\"LEFT\",\"RIGHT\"]\n",
    "        \n",
    "        self.start = (0,0)\n",
    "        self.terminal = (2,2)\n",
    "        self.walls = [(0,1), (1,1), (2,1), (3,1), (3,2), (3,3), (3,4), (2,4)]\n",
    "        \n",
    "        self.prob = 1.0 # change to make the env stochastic, 1=deterministic\n",
    "        \n",
    "        \n",
    "        self.agentColor = 0.3\n",
    "        self.startColor = 0.8\n",
    "        self.terminalColor = 0.5\n",
    "        self.wallColor = 0.1\n",
    "        self.N = 6\n",
    "        self.agentPosition = self.start\n",
    "        \n",
    "        self.world = np.zeros((self.N,self.N))\n",
    "        \n",
    "    def showMap(self):\n",
    "        self.world = np.ones((self.N,self.N))\n",
    "        \n",
    "        if(self.agentPosition != self.start):\n",
    "            self.world[self.start[0], self.start[1]] = self.startColor\n",
    "        if(self.agentPosition != self.terminal):\n",
    "            self.world[self.terminal[0], self.terminal[1]] = self.terminalColor\n",
    "            \n",
    "        self.world[self.agentPosition[0], self.agentPosition[1]] = self.agentColor\n",
    "        \n",
    "        for wall in self.walls:\n",
    "            self.world[wall] = self.wallColor\n",
    "            \n",
    "        fig, ax = plt.subplots(figsize=(self.N*3,self.N))\n",
    "        \n",
    "        ax.imshow(self.world)\n",
    "        ax.grid(which='major', axis='both', linestyle='-', color='k', linewidth=3)\n",
    "        ax.set_xticks(np.arange(-.5, self.N, 1));\n",
    "        ax.set_yticks(np.arange(-.5, self.N, 1));\n",
    "        \n",
    "    def setAgent(self, position):\n",
    "        self.agentPosition = position\n",
    "\n",
    "    def reset(self):\n",
    "        # to do\n",
    "        return self.start\n",
    "    \n",
    "    def probNextStates(self, initState, action):\n",
    "        \"\"\" Return the next state probability for the MDP as a dictionary.\n",
    "\n",
    "        Keyword Arguments:\n",
    "        initState -- The current state s.\n",
    "        action -- The chosen action in state s, a.\n",
    "\n",
    "        \"\"\"\n",
    "        nextStateProbs = {}\n",
    "        if initState != self.terminal:\n",
    "            \n",
    "            # Get all the possible actions\n",
    "            possibleDestinations = [(initState[0], max(0,initState[1]-1)),(initState[0], min(5,initState[1]+1)),\n",
    "                                    (max(0,initState[0]-1), initState[1]), (min(5,initState[0]+1), initState[1])]\n",
    "\n",
    "            for possibleState in possibleDestinations:\n",
    "\n",
    "                if possibleState in self.walls:\n",
    "                    possibleDestinations.remove(possibleState)\n",
    "                    possibleDestinations.append(initState)\n",
    "            \n",
    "            intendedDestination = None\n",
    "            \n",
    "            if action == \"UP\":\n",
    "                intendedDestination = (max(0,initState[0]-1), initState[1])\n",
    "            elif action == \"DOWN\" :\n",
    "                intendedDestination = (min(5,initState[0]+1), initState[1])\n",
    "            elif action == \"LEFT\":\n",
    "                intendedDestination = (initState[0], max(0,initState[1]-1))\n",
    "            else:\n",
    "                intendedDestination = (initState[0], min(5,initState[1]+1))\n",
    "            \n",
    "            if intendedDestination in self.walls:\n",
    "                intendedDestination = initState\n",
    "        \n",
    "            # assign prob to remaining possible states\n",
    "            if len(possibleDestinations) > 1:\n",
    "                otherDestinationProbs = (1.0 - self.prob) / len(possibleDestinations)\n",
    "            \n",
    "            for possibleState in possibleDestinations:\n",
    "                if not possibleState in nextStateProbs.keys():\n",
    "                    nextStateProbs[possibleState] = 0.0\n",
    "                nextStateProbs[possibleState] += otherDestinationProbs\n",
    "                \n",
    "            nextStateProbs[intendedDestination] += self.prob\n",
    "\n",
    "        else:\n",
    "            nextStateProbs = {initState[0]:1.0}\n",
    "            \n",
    "        return nextStateProbs\n",
    "    \n",
    "    def step(self, state, action):\n",
    "        prevState = state\n",
    "        prob_dict = self.probNextStates(state, action)\n",
    "        nextStates = list(prob_dict.keys())\n",
    "        nextStateProb = list(prob_dict.values())\n",
    "        ind = np.random.choice( range(len(nextStates)), p=nextStateProb )\n",
    "        nextState = nextStates[ind]\n",
    "        reward = 0.0\n",
    "        if nextState == self.terminal:\n",
    "            status = 'done'\n",
    "            reward = 1.0\n",
    "#         elif nextState in self.walls:\n",
    "#             status = 'done'\n",
    "#             reward = -1.0\n",
    "        else:\n",
    "            status = 'moving'\n",
    "        \n",
    "        return nextState, reward, status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim is to find an optimal policy that can navigate the agent towards the target location.\n",
    "We will achieve that through the Q-Learning algorithm. The next cell presents the `QLearningAgent()` class. You have to fill in the remaining lines of code and leave the rest of the code as it is. In particular, your aim is to complete the `learn()` method implementing the Q-learning step. \n",
    "\n",
    "Next, you will do the same with the `act()` method. This method will return the optimal action that the agent needs to execute, however, you will implement $\\epsilon$-greedy which allows to balance the exploitation vs exploration dilemma. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent():\n",
    "    def __init__(self, learningRate, discountFactor, epsilon, initVals=0.0):\n",
    "        self.learningRate = learningRate\n",
    "        self.gamma = discountFactor\n",
    "        self.epsilon = epsilon\n",
    "        self.episode_sar = []\n",
    "        \n",
    "        self.possibleActions = [\"UP\",\"DOWN\",\"LEFT\",\"RIGHT\"]\n",
    "        self.terminal = (2,2)\n",
    "\n",
    "\n",
    "        self.allStates = [(x, y) for x in range(6) for y in range(6)]\n",
    "        self.walls = [(0,1), (1,1), (2,1), (3,1), (3,2), (3,3), (3,4), (2,4)]\n",
    "\n",
    "        self.S = list(filter(lambda i: i not in self.walls, self.allStates))\n",
    "        \n",
    "\n",
    "        self.Q = dict.fromkeys(self.S)\n",
    "\n",
    "        # initialise all Q(S,A) to 0.0\n",
    "        for state in self.S:\n",
    "            self.Q[state] = {self.possibleActions[i]: initVals for i in range(len(self.possibleActions))}\n",
    "    \n",
    "    ##############################################\n",
    "    ##\n",
    "    ## Complete the next two methods: learn() and act().\n",
    "    ##\n",
    "    ##############################################\n",
    "    def learn(self):\n",
    "        step = self.episode_sar # state, action, nextState, reward\n",
    "        # step[0] = state, step[1] = action, step[2] = nextState, step[3] = reward\n",
    "\n",
    "        opt_action = 'TODO'\n",
    "        \n",
    "        # compute next state value\n",
    "        max_value = 'TODO' \n",
    "\n",
    "        Q_before = self.Q[step[0]][step[1]]\n",
    "        \n",
    "        # update Q-value\n",
    "        self.Q[step[0]][step[1]] += 'TODO'\n",
    "\n",
    "        return self.Q[step[0]][step[1]]-Q_before\n",
    "\n",
    "    def act(self):\n",
    "        # return action associated with current state, following policy\n",
    "\n",
    "        # give to all action prob of epsilon/possibleActions\n",
    "        action_probs = self.epsilon / len(self.possibleActions)\n",
    "\n",
    "        # dict of action with probabilities {action: probability}\n",
    "        prob_dict = dict.fromkeys(self.possibleActions, action_probs)\n",
    "\n",
    "        opt_action = 'TODO'\n",
    "\n",
    "        max_value = 'TODO'\n",
    "\n",
    "        # opt_dict stores all the equally optimal actions {action: value}\n",
    "        opt_list = []\n",
    "        for action in self.Q[self.curState]:\n",
    "            value = self.Q[self.curState][action]\n",
    "            if value == max_value:\n",
    "                opt_list.append(action)\n",
    "\n",
    "        # select optimal randomly (arbitrary) from all the optimal actions\n",
    "        # shuffle it and pick the first one\n",
    "        np.random.shuffle(opt_list)\n",
    "        opt_action = opt_list[0]\n",
    "\n",
    "        # change the probability of th optimal action\n",
    "        prob_dict[opt_action] = 'TODO'\n",
    "\n",
    "        action_selected = 'TODO'\n",
    "\n",
    "        return action_selected\n",
    "    \n",
    "    ##############################################\n",
    "    ##\n",
    "    ## The End. Leave the remaining code as it is.\n",
    "    ##\n",
    "    ##############################################\n",
    "\n",
    "#     def toStateRepresentation(self, state):\n",
    "# #         self.defendState = state[1]\n",
    "#         return state\n",
    "\n",
    "    def setState(self, state):\n",
    "        self.curState = state\n",
    "\n",
    "    def setExperience(self, state, action, reward, status, nextState):\n",
    "        self.episode_sar = state, action, nextState, reward\n",
    "\n",
    "    def reset(self):\n",
    "        self.episode_sar = []\n",
    "        \n",
    "    def setLearningRate(self, learningRate):\n",
    "        self.learningRate = learningRate\n",
    "\n",
    "    def setEpsilon(self, epsilon):\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def computeHyperparameters(self, numTakenActions, episodeNumber):\n",
    "        # TO-DO\n",
    "        # return alpha, eps\n",
    "        pass\n",
    "\n",
    "    def showQVal(self, N):\n",
    "        world = np.ones((N,N))\n",
    "        world[self.terminal[0], self.terminal[1]] = 0.5\n",
    "        \n",
    "        for wall in self.walls:\n",
    "            world[wall] = 0.1\n",
    "            \n",
    "        fig, ax = plt.subplots(figsize=(N*2,N*2))\n",
    "        \n",
    "        ax.imshow(world)\n",
    "        ax.grid(which='major', axis='both', linestyle='-', color='k', linewidth=3)\n",
    "        ax.set_xticks(np.arange(-.5, N, 1));\n",
    "        ax.set_yticks(np.arange(-.5, N, 1));\n",
    "        \n",
    "        fc = (.2, .5, 1.0)\n",
    "        bbox_propsA = dict(boxstyle=\"round\", fc=\"w\", pad=.1, ec=\"1.0\", alpha=0.0)\n",
    "        bbox_propsR = dict(boxstyle=\"rarrow\", pad=2.0, fc=(.9, .9, .9), ec=\"b\", lw=2)\n",
    "        bbox_props = dict(boxstyle=\"rarrow\", pad=2.0, fc=(.9, .9, .9), ec=\"b\", lw=2,)\n",
    "        bbox_propsL = dict(boxstyle=\"larrow\", pad=2.0, fc=(.9, .9, .9), ec=\"b\", lw=2)\n",
    "        bbox_R_best = dict(boxstyle=\"rarrow\", pad=2.0, fc=(.2, .5, 1.0), ec=\"b\", lw=2)\n",
    "        bbox_UD_best = dict(boxstyle=\"rarrow\", pad=2.0, fc=(.2, .5, 1.0), ec=\"b\", lw=2,)\n",
    "        bbox_L_best = dict(boxstyle=\"larrow\", pad=2.0, fc=(.2, .5, 1.0), ec=\"b\", lw=2)\n",
    "\n",
    "        \n",
    "        for q in self.Q:\n",
    "            if q == self.terminal:\n",
    "                continue\n",
    "            opt_action = max(self.Q[q],key=self.Q[q].get)\n",
    "#             print('{:<20} '.format(opt_action), end=\"\")\n",
    "#             print(\" \")\n",
    "            if opt_action==\"DOWN\":\n",
    "                ax.text(q[1], q[0]+0.3, \" \", ha=\"center\", va=\"center\", size=6, rotation=-90, bbox=bbox_UD_best)\n",
    "            else:\n",
    "                ax.text(q[1], q[0]+0.3, \" \", ha=\"center\", va=\"center\", size=6, rotation=-90, bbox=bbox_props)\n",
    "            \n",
    "            if opt_action==\"UP\":\n",
    "                ax.text(q[1], q[0]-0.3, \" \", ha=\"center\", va=\"center\", size=6, rotation=90, bbox=bbox_UD_best)\n",
    "            else:\n",
    "                ax.text(q[1], q[0]-0.3, \" \", ha=\"center\", va=\"center\", size=6, rotation=90, bbox=bbox_props)\n",
    "            if opt_action==\"RIGHT\":\n",
    "                ax.text(q[1]+0.3, q[0], \" \", ha=\"center\", va=\"center\", size=6, rotation=0, bbox=bbox_R_best)\n",
    "            else:\n",
    "                ax.text(q[1]+0.3, q[0], \" \", ha=\"center\", va=\"center\", size=6, rotation=0, bbox=bbox_propsR)\n",
    "            if opt_action==\"LEFT\":\n",
    "                ax.text(q[1]-0.3, q[0], \" \", ha=\"center\", va=\"center\", size=6, rotation=0, bbox=bbox_L_best)\n",
    "            else:\n",
    "                ax.text(q[1]-0.3, q[0], \" \", ha=\"center\", va=\"center\", size=6, rotation=0, bbox=bbox_propsL)\n",
    "            \n",
    "            \n",
    "            ax.text(q[1], q[0]+0.3, '{:0.2f}'.format(self.Q[q][\"DOWN\"]), ha=\"center\", va=\"center\", size=10,\n",
    "                    rotation=0, bbox=bbox_propsA)\n",
    "            ax.text(q[1], q[0]-0.3, '{:0.2f}'.format(self.Q[q][\"UP\"]), ha=\"center\", va=\"center\", size=10,\n",
    "                    rotation=0, bbox=bbox_propsA)\n",
    "            ax.text(q[1]+0.3, q[0], '{:0.2f}'.format(self.Q[q][\"RIGHT\"]), ha=\"center\", va=\"center\", size=10,\n",
    "                    rotation=0, bbox=bbox_propsA)\n",
    "            ax.text(q[1]-0.3, q[0], '{:0.2f}'.format(self.Q[q][\"LEFT\"]), ha=\"center\", va=\"center\", size=10,\n",
    "                    rotation=0, bbox=bbox_propsA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having implemented the previous code, run the next cell to initialise the environment and the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = MDP()\n",
    "state = environment.reset()\n",
    "environment.showMap()\n",
    "\n",
    "# Initialize a Q-Learning Agent\n",
    "agent = QLearningAgent(learningRate = 0.1, discountFactor = 0.9, epsilon = 0.01)\n",
    "numEpisodes = 1000 #20000\n",
    "\n",
    "\n",
    "numTakenActions = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run the following cell to start training Q-learning agent, leave the uncommented part as it is, we will return to it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training using Q-Learning\n",
    "\n",
    "plt.figure(figsize=(15,9))\n",
    "\n",
    "numActionsList = []\n",
    "episodeAction = 0\n",
    "\n",
    "for episode in range(numEpisodes):\n",
    "    status = 0\n",
    "    episodeAction = 0\n",
    "    observation = environment.reset()\n",
    "    status = 'moving'\n",
    "\n",
    "    while status=='moving':\n",
    "        ###### tune hyperparameters - leave it for later #######\n",
    "        # learningRate, epsilon = agent.computeHyperparameters(numTakenActions, episode)\n",
    "        # agent.setEpsilon(epsilon)\n",
    "        # agent.setLearningRate(learningRate)\n",
    "        ########################################################\n",
    "        obsCopy = copy.copy(observation)\n",
    "        agent.setState(obsCopy)\n",
    "        action = agent.act()\n",
    "        numTakenActions += 1\n",
    "        episodeAction += 1\n",
    "\n",
    "        nextObservation, reward, status = environment.step(obsCopy, action)\n",
    "        agent.setExperience(obsCopy, action, reward, status, nextObservation)\n",
    "        update = agent.learn()\n",
    "\n",
    "        observation = nextObservation\n",
    "        \n",
    "        # commenting as it slows down the overall process \n",
    "        # environment.setAgent(position=observation)\n",
    "        # environment.showMap()\n",
    "    \n",
    "    numActionsList.append(episodeAction)\n",
    "    \n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(numActionsList)\n",
    "plt.title('Number of total actions taken at each episode')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(numActionsList[-300:]) # select last N episodes \n",
    "plt.title('Number of total actions taken at the last N episodes')\n",
    "\n",
    "# show the environment with each Q-value as well as the optimal Q-value.\n",
    "agent.showQVal(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zoom more in the last $N$ episodes and check how the number of actions changes over the episodes.\n",
    "\n",
    "What is the number of optimal actions? You might observe some jumps, why are you observing this behaviour?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "\n",
    "plt.figure(figsize=(15,9))\n",
    "# plt.clf()\n",
    "plt.subplot(2,2,1)\n",
    "# plt.cla()\n",
    "plt.plot(numActionsList)\n",
    "# plt.imshow()\n",
    "plt.title('Number of total actions taken at each episode')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(numActionsList[-N:]) # select last N episodes \n",
    "plt.title('Number of total actions taken at the last N episodes')\n",
    "\n",
    "# show the environment with each Q-value as well as the optimal Q-value.\n",
    "agent.showQVal(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 What is the role of the different hyperparameters?\n",
    "\n",
    "In this section we will ask you to test different hyperparameters. Lets first explore the role of the learning rate $\\alpha$. Try $\\alpha = 0.01$ and $\\alpha=1$. What do you expect? And do you get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TODO'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add code here to test different learning rates (alpha)\n",
    "'TODO'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1.1 Discount factor\n",
    "Now lets explore the role of the discount factor $\\gamma$. Try $\\gamma = 0.01$ and $\\gamma=1$. What do you expect? And do you get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add code here to test different discount factors (gamma)\n",
    "'TODO'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1.2 Exploration vs exploitation (epsilon)\n",
    "Now lets explore the role of $\\epsilon$ in $\\epsilon$-greedy policy. Try $\\epsilon = 0.01$ and $\\epsilon=1$. What do you expect? And do you get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TODO'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add code here to test different epsilons\n",
    "'TODO'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1.3 Implement a meta hyperparameter function\n",
    "Go back and uncomment the `tune hyperparameter` block. Implement the `computeHyperparameters()` function of the agent such that you can optimise the hyperparameters ($\\alpha, \\epsilon$) according to the number of actions taken and episode.  What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Congratulations, you have reached the end of this lab. Let's summarise what we have learnt.\n",
    "* Understand temporal difference error and its relation to the learning process in the brain.\n",
    "* Understand the difference between Value and Q-value functions as well as other key components of the reinforcement learning framework.\n",
    "* Main steps behind Q-learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* COMSM0075 Information Processing and the Brain: Lecture 4, Reinforcement Learning\n",
    "* Sutton and Barto: Reinforcement Learning - An Introduction, Chapter 6 Temporal-Difference Learning\n",
    "* [Neuromatch Academy 2020](https://www.neuromatchacademy.org)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
