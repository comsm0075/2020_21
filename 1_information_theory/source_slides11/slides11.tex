
\documentclass{beamer}
\usepackage[latin1]{inputenc}
%\usetheme{Montpellier}
%\usetheme{Boadilla}
%\usecolortheme[RGB={204,51,255}]{structure}
%\usecolortheme[named=purple]{structure}
\usecolortheme[RGB={62,128,62}]{structure}
%\definecolor{reddish}{rgb}{0.3,0.15,0.3}
%\definecolor{light}{rgb}{0.8,0.6,0.8}
%\definecolor{reddish}{rgb}{.5,0.15,0.15}
\definecolor{reddish}{rgb}{0.5,0.3,0.4}
%\definecolor{light}{rgb}{0.8,0.6,0.8}
\definecolor{reddish}{rgb}{.7,0.25,0.25}
\definecolor{greenish}{rgb}{.25,0.8,0.25}
\definecolor{blueish}{rgb}{.25,0.25,0.7}
\definecolor{purple}{rgb}{.5,0.0,0.5}
\usepackage{graphicx}
\usepackage{pstricks}
\usepackage{epsfig}
\newcommand{\btVFill}{\vskip0pt plus 1filll}

\setbeamertemplate{navigation symbols}{}

\newcommand{\crish}{\color{reddish}}
\newcommand{\cgish}{\color{greenish}}
\newcommand{\cbla}{\color{black}}
\newcommand{\cred}{\color{red}}
\newcommand{\cblu}{\color{blue}}
\newcommand{\cgrish}{\color{green}}

\newcommand{\sm}{\color{reddish}$}
\newcommand{\fm}{$\color{black}{}}

\newcommand{\letter}[1]{\color{blue}\texttt{#1}\color{black}}
\newcommand{\binary}[1]{\color{red}\texttt{#1}\color{black}}

\usepackage{tikz}
\usetikzlibrary{arrows,decorations.markings,positioning}
\usepackage{epstopdf}
\usetikzlibrary{fit}
\usepackage{pgfplots}

\title[Information Theory lecture 10]{Infomax: information theory lecture 10}
\author{COMSM0075 Information Processing and Brain}
\institute{\texttt{comsm0075.github.io}}
\date{October 2020}

\begin{document}

\maketitle

\begin{frame}{Source separation}
  \cred
  $$
 \mathbf{ s}\stackrel{\mbox{mixing}}{\longrightarrow}\mathbf{r}=M\mathbf{ s}\stackrel{\mbox{unmixing}}{\longrightarrow}\mathbf{ x}=W\mathbf{r}
 $$
 \cbla
\end{frame}

\begin{frame}{Mutual information}
  Two-dimensional case: we are assuming the two sources \crish$S_1$\cbla{} and \crish$S_2$\cbla{} are independent, so we want to find independent \crish$X_1$\cbla{} and \crish$X_2$\cbla.
\end{frame}

\begin{frame}{Mutual information}
  Two-dimensional case: we are assuming the two sources \crish$S_1$\cbla{} and \crish$S_2$\cbla{} are independent, so we want to find independent \crish$X_1$\cbla{} and \crish$X_2$\cbla:
  \crish
  $$
  I(X_1,X_2)=0
  $$
  \cbla
  or at the very least we'll try to minimize \cblu$I(X_1,X_2)$\cbla{}.
\end{frame}


\begin{frame}{Mutual information}
  Two-dimensional case: we are assuming the two sources \crish$S_1$\cbla{} and \crish$S_2$\cbla{} are independent, so we want to find independent \crish$X_1$\cbla{} and \crish$X_2$\cbla:
  \crish
  $$
  I(X_1,X_2)=0
  $$
  \cbla
  or at the very least we'll try to minimize \cblu$I(X_1,X_2)$\cbla{}.
\end{frame}


\begin{frame}{Infomax}
  We want to minimize \cblu$I(X_1,X_2)$\cbla{} but this is very hard to calculate!
  \crish
  $$
  I(X_1,X_2)=H(X_1)+H(X_2)-H(X_1,X_2)
  $$
  \cbla
  Let's maximize \crish$H(X_1,X_2)$\cbla{} instead.
  
\end{frame}


\begin{frame}{Infomax}
  We want to minimize $I(X_1,X_2)$\cbla{} but this is very hard to calculate!
  \crish
  $$
  I(X_1,X_2)=H(X_1)+H(X_2)-H(X_1,X_2)
  $$
  \cbla
  Let's \cblu{}maximize $H(X_1,X_2)$\cbla{} instead.
  \end{frame}



\begin{frame}{Infomax}
  We want to minimize $I(X_1,X_2)$\cbla{} but this is very hard to calculate!
  \crish
  $$
  I(X_1,X_2)=H(X_1)+H(X_2)-H(X_1,X_2)
  $$
  \cbla
  Let's maximize \crish$H(X_1,X_2)$\cbla{} instead.
  \begin{itemize}
  \item This means ignoring \crish$H(X_1)$\cbla{} and \crish$H(X_2)$\cbla{}.
  \item It isn't obvious \crish$H(X_1,X_2)$\cbla{} is any easier to calculate than \crish$I(X_1,X_2)$\cbla{}.
  \end{itemize}
\end{frame}

\begin{frame}{An obvious problem}
  The differential entropy isn't scale invariant
  \crish
  $$
  H(\cblu{}\lambda\crish{}{}X_1,X_2)=H(X_1,X_2)+\log_2{|\cblu{}\lambda\crish{}|}
  $$
  \cbla
  so it tells us nothing about mixing and unmixing.
  \end{frame}


\begin{frame}{An obvious problem}
  The differential entropy isn't scale invariant
  \crish
  $$
  H(X_1,\cblu{}\lambda\crish{}{}X_2)=H(X_1,X_2)+\log_2{|\cblu{}\lambda\crish{}|}
  $$
  \cbla
    so it tells us nothing about mixing and unmixing.
  \end{frame}

\begin{frame}{An obvious problem}
  The differential entropy isn't scale invariant
  \crish
  $$
  H(\cblu{}\lambda\crish{}{}X_1,\cblu{}\lambda\crish{}{}X_2)=H(X_1,X_2)+2\log_2{|\cblu{}\lambda\crish{}|}
  $$
  \cbla
    so it tells us nothing about mixing and unmixing.
\end{frame}

\begin{frame}{A very clever solution}
  Inspire by the behaviour of neurons Bell and Sejnowski added a saturating non-linearity:
  \crish
  \begin{eqnarray*}
y_1&=&g(x_1+w_1)\cr
y_2&=&g(x_2+w_2)
\end{eqnarray*}
  \cbla
  where  \crish$w_1$\cbla{} and \crish$w_2$\cbla{} are parameters and, for example,
\crish $$
g(u)=\frac{1}{1+e^{-u}}
$$ \cbla{}
is a saturating non-linearity so \crish$g:(-\infty,\infty)\rightarrow (0,1)$.\cbla{}
  \vfill
  \flushright{\tiny{Bell, A. J. and Sejnowski, T. J. (1995)}}
\end{frame}


\begin{frame}{Saturating non-linearity}

\crish $$
g(u)=\frac{1}{1+e^{-u}}
$$ \cbla{}
is a saturating non-linearity so \crish$g:(-\infty,\infty)\rightarrow (0,1)$.\cbla{}
\begin{center}
  \epsfig{file=g.eps,width=4cm,angle=270}
\end{center}
\end{frame}

\begin{frame}{Saturating non-linearity}
  Say \crish$X$\cbla{} is uniform
    \cbla
  \begin{center}
    \begin{tikzpicture}
    \node[](left){$\ldots$};
    \node[right =  1cm of left](zero){};
    \node[above = 2cm of zero](overzero){};
    \node[right = 0.5cm of overzero](overa){};
    \node[below = 2cm of overa](a){};
    \node[right = 1cm of a](right){$\ldots$};
    \draw[thick] (left) -- (zero.center) -- (overzero.center) -- (overa.center) -- (a.center) -- (right);
    \draw[dotted] (zero.center) -- (a.center);
    \node[below = 0.01 cm of zero.west](belowzero){$-\frac{a}{2}$};
    \node[below = 0.01 cm of a.center](belowa){$\frac{a}{2}$};
    \node[left = 0.0675 cm of overzero](leftoverzero){$\frac{1}{a}$};
    \end{tikzpicture}
  \end{center}
  \end{frame}


\begin{frame}{Saturating non-linearity}
  Now calculate \crish$p_G(g)$\cbla{}
\begin{center}
  \include{g_output}
  \end{center}

\end{frame}


\begin{frame}{Saturating non-linearity}
  Now calculate \crish$p_G(g)$\cbla{}
\begin{center}
  \begin{tabular}{ll}
    $a=1$&$h(G)=-1.41$\\
    $a=5$&$h(G)=-0.26$\\
    $a=10$&$h(G)=-1.03$\\
    $a=15$&$h(G)=-11.6$
    \end{tabular}
  \end{center}

\end{frame}


\end{document}

