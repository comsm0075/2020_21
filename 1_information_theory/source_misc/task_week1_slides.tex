
\documentclass{beamer}
\usepackage[latin1]{inputenc}
%\usetheme{Montpellier}
%\usetheme{Boadilla}
%\usecolortheme[RGB={204,51,255}]{structure}
%\usecolortheme[named=purple]{structure}
\usecolortheme[RGB={62,128,62}]{structure}
%\definecolor{dark}{rgb}{0.3,0.15,0.3}
%\definecolor{light}{rgb}{0.8,0.6,0.8}
%\definecolor{reddish}{rgb}{.5,0.15,0.15}
\definecolor{dark}{rgb}{0.5,0.3,0.4}
%\definecolor{light}{rgb}{0.8,0.6,0.8}
\definecolor{reddish}{rgb}{.7,0.25,0.25}
\definecolor{greenish}{rgb}{.25,0.7,0.25}
\definecolor{blueish}{rgb}{.25,0.25,0.7}
\definecolor{purple}{rgb}{.5,0.0,0.5}
\usepackage{graphicx}
\usepackage{pstricks}

\setbeamertemplate{navigation symbols}{}

\newcommand{\crish}{\color{reddish}}
\newcommand{\cbla}{\color{black}}
\newcommand{\cred}{\color{red}}
\newcommand{\cblu}{\color{blue}}
\newcommand{\cgre}{\color{green}}

\newcommand{\sm}{\color{reddish}$}
\newcommand{\fm}{$\color{black}{}}

\newcommand{\letter}[1]{\color{blue}\texttt{#1}\color{black}}
\newcommand{\binary}[1]{\color{red}\texttt{#1}\color{black}}

\usepackage{tikz}
\usetikzlibrary{arrows,decorations.markings,positioning}
\usepackage{epstopdf}

\title[Shannon's Entropy]{Shannon's Enropy}
\author{COMSM0075 Information Processing and Brain}
\institute{\texttt{comsm0075.github.io}}
\date{September 2020}

\begin{document}

\maketitle

\begin{frame}{Shannon's entropy}
  For a finite discrete distribution with random variable \sm X\fm,
  possible outcomes \sm\{x_1,x_2,\ldots x_n\}\in\mathcal{X}\fm{} and a
  probability mass function \sm p_X\fm{} giving probabilities \sm p_X(x_i)\fm, the
  entropy is
\crish
  $$
H(X)=-\sum_{x_i\in \mathcal{X}}{p_X(x_i)\log_2p_X(x_i)}
  $$
\cbla
\end{frame}

\begin{frame}{Shannon's entropy}
  The probabilities can be estimated
  \crish
  $$
  p(x_i)\approx \frac{f(x_i)}{\sum_i f(x_i)}
  $$
  \cbla
  where \sm f(x_i)\fm{} is the number of times \sm x_i\fm{} occurs.
\end{frame}


\begin{frame}{Biased estimation}
Note that this gives a biased estimator. If all events are equally
likely then \sm{}p(x_i)=1/n\fm{} but the estimates of the probability
won't give equal estimates and so if \sm{}H(X)=\log_2{n}\fm{},
estimates of \sm{}H(X)\fm{} from data will typically give a lower
value.  
\end{frame}

\begin{frame}{Biased estimation}
An articifial experiment could be done to calculate the average
estimate of \sm{}H(X)\fm{} for a particular \sm{}n\fm{} and a particular number of samples.
\end{frame}


\end{document}

