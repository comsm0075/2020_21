\documentclass[12pt]{article}
\usepackage{amsfonts, epsfig}
\usepackage[authoryear]{natbib}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{slashbox}
\pagestyle{fancy}
\lfoot{\texttt{comsm0075.github.io}}
\lhead{IP\&B Worksheet 3 - Conor}
\rhead{\thepage}
\cfoot{}
\begin{document}

\section*{Worksheet 3 - differential entropy} 

This problem are taken from the excellent text book Cover and
Thomas. It is actually hard to find reasonably doable questions on
differential entropy and these would be hard to get done in time in an
exam.

\subsection*{Q1 - differential entropy}

What is the entropy of $X_1+X_2$ where $X_1$ and $X_2$ are independent normal variables with means $\mu_i$ and variances $\sigma_i^2$?



\subsection*{Q2 - channel capacity}

If $Z=X+Y$ where $X$ is uniform on $[-1/2,1/2]$ and $Y$ is independent of $X$ and uniform on $[-a/2,a/2]$ with $a<1$, what is $I(X;Z)$ as a function of $a$?

\newpage

\subsection*{Q1 - outline solution}

You can use the convolution,
\begin{equation}
  p_Z(z)=\int_{-\infty}^\infty p_X(x)p_Y(z-x)dx
\end{equation}
for $Z=X+Y$, to show that the sum of two Gaussians gives another Gaussian with $\mu=\mu_1+\mu_2$ and $\sigma^2=\sigma_1^2+\sigma_2^2$; you can then apply the usual formula for the entropy of a Gaussian:
\begin{equation}
  h(X_1+X_2)=\frac{1}{2}\log{2\pi e \sigma^2}
\end{equation}

\subsection*{Q2 - outline solution}

So for a random variable uniform on a region of width $a$, say $[y_0,y_0+a]$
\begin{equation}
  h(Y)=-\frac{1}{a}\int_{y_0}^{y_o+a}\log{\frac{1}{a}}dy=\log{a}
\end{equation}
Now working out $p_Z(z)$ is harder because, again using the convolution we get
\begin{equation}
  p_Z(z)=\left\{\begin{array}{ll}
  \frac{1}{a}\left(z+\frac{1+a}{2}\right)&-\frac{1+a}{2}<z<-\frac{1-a}{2}\\
  1                                       &-\frac{1-a}{2}<z<\frac{1-a}{2}\\
  \frac{1}{a}\left(-z+\frac{1+a}{2}\right)&\frac{1-a}{2}<z<\frac{1+a}{2}\\
  0&\mbox{otherwise}
  \end{array}\right.
\end{equation}
This would allow $h(Z)$ to be calculated by splitting up the integral and doing the relevant integrations by parts. This gives
\begin{equation}
  h(Z)=\frac{a}{2}
\end{equation}
Finally if $X$ is fixed $Z$ is just the uniform distribution so
\begin{equation}
  h(Z|X)=\log{a}
\end{equation}
and hence
\begin{equation}
  I(Z;X)=h(Z)-h(Z|X)=\frac{a}{2}-\log{a}
\end{equation}



\end{document}

